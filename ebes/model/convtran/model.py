from torch import nn
from .AbsolutePositionalEncoding import (
    tAPE,
    AbsolutePositionalEncoding,
    LearnablePositionalEncoding,
)
from .Attention import Attention, Attention_Rel_Scl, Attention_Rel_Vec
from ..basemodel import BaseModel
from ...types import Seq


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


class Permute(nn.Module):
    def forward(self, x):
        return x.permute(1, 0, 2)


class ConvTran(BaseModel):
    def __init__(
        self,
        channel_size,
        seq_len,
        num_classes,
        emb_size=64,
        num_heads=8,
        dropout=0.01,
    ):
        super().__init__()
        self.seq_len = seq_len
        # Parameters Initialization -----------------------------------------------
        self.Fix_pos_encode = "tAPE"
        self.Rel_pos_encode = "eRPE"
        # Embedding Layer -----------------------------------------------------------
        self.embed_layer = nn.Sequential(
            nn.Conv2d(1, emb_size * 4, kernel_size=[1, 8], padding="same"),
            nn.BatchNorm2d(emb_size * 4),
            nn.GELU(),
        )

        self.embed_layer2 = nn.Sequential(
            nn.Conv2d(
                emb_size * 4, emb_size, kernel_size=[channel_size, 1], padding="valid"
            ),
            nn.BatchNorm2d(emb_size),
            nn.GELU(),
        )

        if self.Fix_pos_encode == "tAPE":
            self.Fix_Position = tAPE(emb_size, dropout=dropout, max_len=seq_len)
        elif self.Fix_pos_encode == "Sin":
            self.Fix_Position = AbsolutePositionalEncoding(
                emb_size, dropout=dropout, max_len=seq_len
            )
        elif self.Fix_pos_encode == "Learn":
            self.Fix_Position = LearnablePositionalEncoding(
                emb_size, dropout=dropout, max_len=seq_len
            )

        if self.Rel_pos_encode == "eRPE":
            self.attention_layer = Attention_Rel_Scl(
                emb_size, num_heads, seq_len, dropout
            )
        elif self.Rel_pos_encode == "Vector":
            self.attention_layer = Attention_Rel_Vec(
                emb_size, num_heads, seq_len, dropout
            )
        else:
            self.attention_layer = Attention(emb_size, num_heads, dropout)

        self.LayerNorm = nn.LayerNorm(emb_size, eps=1e-5)
        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)

        self.FeedForward = nn.Sequential(
            nn.Linear(emb_size, emb_size * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(emb_size * 4, emb_size),
            nn.Dropout(dropout),
        )

        self.gap = nn.AdaptiveAvgPool1d(1)
        self.flatten = nn.Flatten()
        self.out = nn.Linear(emb_size, num_classes)

    def forward(self, seq: Seq):
        # Required formatt:
        x = seq.tokens.permute(1, 2, 0)
        x = nn.functional.pad(x, (0, self.seq_len - x.shape[2]))
        # Initiall code:
        x = x.unsqueeze(1)
        x_src = self.embed_layer(x)
        x_src = self.embed_layer2(x_src).squeeze(2)
        x_src = x_src.permute(0, 2, 1)
        if self.Fix_pos_encode != "None":
            x_src_pos = self.Fix_Position(x_src)
            att = x_src + self.attention_layer(x_src_pos)
        else:
            att = x_src + self.attention_layer(x_src)
        att = self.LayerNorm(att)
        out = att + self.FeedForward(att)
        out = self.LayerNorm2(out)
        out = out.permute(0, 2, 1)
        out = self.gap(out)
        out = self.flatten(out)
        out = self.out(out)
        return out


class CasualConvTran(nn.Module):
    def __init__(self, config, num_classes):
        super().__init__()
        # Parameters Initialization -----------------------------------------------
        channel_size, seq_len = config["Data_shape"][1], config["Data_shape"][2]
        emb_size = config["emb_size"]
        num_heads = config["num_heads"]
        dim_ff = config["dim_ff"]
        self.Fix_pos_encode = config["Fix_pos_encode"]
        self.Rel_pos_encode = config["Rel_pos_encode"]
        # Embedding Layer -----------------------------------------------------------
        self.causal_Conv1 = nn.Sequential(
            CausalConv1d(channel_size, emb_size, kernel_size=8, stride=2, dilation=1),
            nn.BatchNorm1d(emb_size),
            nn.GELU(),
        )

        self.causal_Conv2 = nn.Sequential(
            CausalConv1d(emb_size, emb_size, kernel_size=5, stride=2, dilation=2),
            nn.BatchNorm1d(emb_size),
            nn.GELU(),
        )

        self.causal_Conv3 = nn.Sequential(
            CausalConv1d(emb_size, emb_size, kernel_size=3, stride=2, dilation=2),
            nn.BatchNorm1d(emb_size),
            nn.GELU(),
        )

        if self.Fix_pos_encode == "tAPE":
            self.Fix_Position = tAPE(
                emb_size, dropout=config["dropout"], max_len=seq_len
            )
        elif self.Fix_pos_encode == "Sin":
            self.Fix_Position = tAPE(
                emb_size, dropout=config["dropout"], max_len=seq_len
            )
        elif config["Fix_pos_encode"] == "Learn":
            self.Fix_Position = LearnablePositionalEncoding(
                emb_size, dropout=config["dropout"], max_len=seq_len
            )

        if self.Rel_pos_encode == "eRPE":
            self.attention_layer = Attention_Rel_Scl(
                emb_size, num_heads, seq_len, config["dropout"]
            )
        elif self.Rel_pos_encode == "Vector":
            self.attention_layer = Attention_Rel_Vec(
                emb_size, num_heads, seq_len, config["dropout"]
            )
        else:
            self.attention_layer = Attention(emb_size, num_heads, config["dropout"])

        self.LayerNorm = nn.LayerNorm(emb_size, eps=1e-5)
        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)

        self.FeedForward = nn.Sequential(
            nn.Linear(emb_size, dim_ff),
            nn.ReLU(),
            nn.Dropout(config["dropout"]),
            nn.Linear(dim_ff, emb_size),
            nn.Dropout(config["dropout"]),
        )

        self.gap = nn.AdaptiveAvgPool1d(1)
        self.flatten = nn.Flatten()
        self.out = nn.Linear(emb_size, num_classes)

    def forward(self, x):
        x = x.unsqueeze(1)
        x_src = self.embed_layer(x)
        x_src = self.embed_layer2(x_src).squeeze(2)
        x_src = x_src.permute(0, 2, 1)
        if self.Fix_pos_encode != "None":
            x_src_pos = self.Fix_Position(x_src)
            att = x_src + self.attention_layer(x_src_pos)
        else:
            att = x_src + self.attention_layer(x_src)
        att = self.LayerNorm(att)
        out = att + self.FeedForward(att)
        out = self.LayerNorm2(out)
        out = out.permute(0, 2, 1)
        out = self.gap(out)
        out = self.flatten(out)
        out = self.out(out)
        return out


class CausalConv1d(nn.Conv1d):
    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size,
        stride=1,
        dilation=1,
        groups=1,
        bias=True,
    ):
        super().__init__(
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=0,
            dilation=dilation,
            groups=groups,
            bias=bias,
        )

        self.__padding = (kernel_size - 1) * dilation

    def forward(self, x):
        return super().forward(nn.functional.pad(x, (self.__padding, 0)))


class Transformer(nn.Module):
    def __init__(self, config, num_classes):
        super().__init__()
        # Parameters Initialization -----------------------------------------------
        channel_size, seq_len = config["Data_shape"][1], config["Data_shape"][2]
        emb_size = config["emb_size"]
        num_heads = config["num_heads"]
        dim_ff = config["dim_ff"]
        self.Fix_pos_encode = config["Fix_pos_encode"]
        self.Rel_pos_encode = config["Rel_pos_encode"]
        # Embedding Layer -----------------------------------------------------------
        self.embed_layer = nn.Sequential(
            nn.Linear(channel_size, emb_size), nn.LayerNorm(emb_size, eps=1e-5)
        )

        if self.Fix_pos_encode == "Sin":
            self.Fix_Position = tAPE(
                emb_size, dropout=config["dropout"], max_len=seq_len
            )
        elif config["Fix_pos_encode"] == "Learn":
            self.Fix_Position = LearnablePositionalEncoding(
                emb_size, dropout=config["dropout"], max_len=seq_len
            )

        self.LayerNorm1 = nn.LayerNorm(emb_size, eps=1e-5)
        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)
        if self.Rel_pos_encode == "Scalar":
            self.attention_layer = Attention_Rel_Scl(
                emb_size, num_heads, seq_len, config["dropout"]
            )
        elif self.Rel_pos_encode == "Vector":
            self.attention_layer = Attention_Rel_Vec(
                emb_size, num_heads, seq_len, config["dropout"]
            )
        else:
            self.attention_layer = Attention(emb_size, num_heads, config["dropout"])

        self.FeedForward = nn.Sequential(
            nn.Linear(emb_size, dim_ff),
            nn.ReLU(),
            nn.Dropout(config["dropout"]),
            nn.Linear(dim_ff, emb_size),
            nn.Dropout(config["dropout"]),
        )

        self.gap = nn.AdaptiveAvgPool1d(1)
        self.flatten = nn.Flatten()
        self.out = nn.Linear(emb_size, num_classes)

    def forward(self, x):
        x_src = self.embed_layer(x.permute(0, 2, 1))
        if self.Fix_pos_encode != "None":
            x_src = self.Fix_Position(x_src)
        att = x_src + self.attention_layer(x_src)
        att = self.LayerNorm1(att)
        out = att + self.FeedForward(att)
        out = self.LayerNorm2(out)

        out = out.permute(0, 2, 1)
        out = self.gap(out)
        out = self.flatten(out)
        out = self.out(out)
        # out = out.permute(1, 0, 2)
        # out = self.out(out[-1])

        return out
